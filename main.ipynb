{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchrec 0.1.0\n",
      "Uninstalling torchrec-0.1.0:\n",
      "  Successfully uninstalled torchrec-0.1.0\n",
      "Found existing installation: torch 2.5.1\n",
      "Uninstalling torch-2.5.1:\n",
      "  Successfully uninstalled torch-2.5.1\n",
      "\u001b[33mWARNING: Skipping fbgemm_gpu as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: torchmetrics 1.0.3\n",
      "Uninstalling torchmetrics-1.0.3:\n",
      "  Successfully uninstalled torchmetrics-1.0.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip3 uninstall -y torchrec torch fbgemm_gpu torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement fbgemm_gpu (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for fbgemm_gpu\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting torchmetrics==1.0.3\n",
      "  Downloading torchmetrics-1.0.3-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting numpy>1.20.0 (from torchmetrics==1.0.3)\n",
      "  Downloading numpy-2.2.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting torch>=1.8.1 (from torchmetrics==1.0.3)\n",
      "  Using cached torch-2.5.1-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/isl-news-rec/lib/python3.12/site-packages (from torchmetrics==1.0.3) (24.2)\n",
      "Collecting lightning-utilities>=0.7.0 (from torchmetrics==1.0.3)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/isl-news-rec/lib/python3.12/site-packages (from lightning-utilities>=0.7.0->torchmetrics==1.0.3) (75.1.0)\n",
      "Collecting typing-extensions (from lightning-utilities>=0.7.0->torchmetrics==1.0.3)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting filelock (from torch>=1.8.1->torchmetrics==1.0.3)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting networkx (from torch>=1.8.1->torchmetrics==1.0.3)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.8.1->torchmetrics==1.0.3)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch>=1.8.1->torchmetrics==1.0.3)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.8.1->torchmetrics==1.0.3)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.8.1->torchmetrics==1.0.3)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.8.1->torchmetrics==1.0.3)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Downloading numpy-2.2.1-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.5.1-cp312-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, numpy, networkx, MarkupSafe, fsspec, filelock, lightning-utilities, jinja2, torch, torchmetrics\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.12.0 jinja2-3.1.5 lightning-utilities-0.11.9 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.1 sympy-1.13.1 torch-2.5.1 torchmetrics-1.0.3 typing-extensions-4.12.2\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torchrec\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchrec-1.0.0%2Bcu121-py3-none-any.whl (604 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m604.5/604.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of torchrec to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchrec-0.8.0%2Bcu121-py3-none-any.whl (521 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchrec-0.7.0%2Bcu121-py3-none-any.whl (467 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchrec-0.6.0%2Bcu121-py3-none-any.whl (429 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchrec-0.5.0%2Bcu121-py3-none-any.whl (393 kB)\n",
      "  Downloading https://download.pytorch.org/whl/torchrec-0.1.1-py39-none-any.whl (182.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Cannot install torchrec==0.1.1, torchrec==0.5.0+cu121, torchrec==0.6.0+cu121, torchrec==0.7.0+cu121, torchrec==0.8.0+cu121 and torchrec==1.0.0+cu121 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    torchrec 1.0.0+cu121 depends on fbgemm-gpu\n",
      "    torchrec 0.8.0+cu121 depends on fbgemm-gpu\n",
      "    torchrec 0.7.0+cu121 depends on fbgemm-gpu\n",
      "    torchrec 0.6.0+cu121 depends on fbgemm-gpu\n",
      "    torchrec 0.5.0+cu121 depends on fbgemm-gpu\n",
      "    torchrec 0.1.1 depends on arrow\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install --pre torch --index-url https://download.pytorch.org/whl/cu121 -U\n",
    "!pip3 install fbgemm_gpu \n",
    "!pip3 install torchmetrics==1.0.3\n",
    "!pip3 install torchrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[0.6035, 0.6354, 0.6695, 0.9251],\n",
      "        [0.3970, 0.9944, 0.5983, 0.2472],\n",
      "        [0.7403, 0.2099, 0.8162, 0.1439],\n",
      "        [0.7578, 0.7182, 0.7899, 0.7837],\n",
      "        [0.1514, 0.2065, 0.7547, 0.8430],\n",
      "        [0.2876, 0.6487, 0.9473, 0.9417],\n",
      "        [0.5384, 0.0167, 0.9074, 0.0474],\n",
      "        [0.1900, 0.1105, 0.0069, 0.4224],\n",
      "        [0.7075, 0.7861, 0.5851, 0.6946],\n",
      "        [0.5893, 0.0765, 0.7783, 0.2476]])\n",
      "Embedding Collection Table:  Parameter containing:\n",
      "tensor([[0.6035, 0.6354, 0.6695, 0.9251],\n",
      "        [0.3970, 0.9944, 0.5983, 0.2472],\n",
      "        [0.7403, 0.2099, 0.8162, 0.1439],\n",
      "        [0.7578, 0.7182, 0.7899, 0.7837],\n",
      "        [0.1514, 0.2065, 0.7547, 0.8430],\n",
      "        [0.2876, 0.6487, 0.9473, 0.9417],\n",
      "        [0.5384, 0.0167, 0.9074, 0.0474],\n",
      "        [0.1900, 0.1105, 0.0069, 0.4224],\n",
      "        [0.7075, 0.7861, 0.5851, 0.6946],\n",
      "        [0.5893, 0.0765, 0.7783, 0.2476]], requires_grad=True)\n",
      "Embedding Bag Collection Table:  Parameter containing:\n",
      "tensor([[0.6035, 0.6354, 0.6695, 0.9251],\n",
      "        [0.3970, 0.9944, 0.5983, 0.2472],\n",
      "        [0.7403, 0.2099, 0.8162, 0.1439],\n",
      "        [0.7578, 0.7182, 0.7899, 0.7837],\n",
      "        [0.1514, 0.2065, 0.7547, 0.8430],\n",
      "        [0.2876, 0.6487, 0.9473, 0.9417],\n",
      "        [0.5384, 0.0167, 0.9074, 0.0474],\n",
      "        [0.1900, 0.1105, 0.0069, 0.4224],\n",
      "        [0.7075, 0.7861, 0.5851, 0.6946],\n",
      "        [0.5893, 0.0765, 0.7783, 0.2476]], requires_grad=True)\n",
      "Input row IDS:  tensor([[1, 3]])\n",
      "Embedding Collection Results: \n",
      "tensor([[[0.3970, 0.9944, 0.5983, 0.2472],\n",
      "         [0.7578, 0.7182, 0.7899, 0.7837]]], grad_fn=<EmbeddingBackward0>)\n",
      "Shape:  torch.Size([1, 2, 4])\n",
      "Embedding Bag Collection Results: \n",
      "tensor([[0.5774, 0.8563, 0.6941, 0.5155]], grad_fn=<EmbeddingBagBackward0>)\n",
      "Shape:  torch.Size([1, 4])\n",
      "Mean:  tensor([[0.5774, 0.8563, 0.6941, 0.5155]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "num_embeddings, embedding_dim = 10, 4\n",
    "\n",
    "# Initialize our embedding table\n",
    "weights = torch.rand(num_embeddings, embedding_dim)\n",
    "print(\"Weights:\", weights)\n",
    "\n",
    "# Pass in pre-generated weights just for example, typically weights are randomly initialized\n",
    "embedding_collection = torch.nn.Embedding(\n",
    "    num_embeddings, embedding_dim, _weight=weights\n",
    ")\n",
    "embedding_bag_collection = torch.nn.EmbeddingBag(\n",
    "    num_embeddings, embedding_dim, _weight=weights\n",
    ")\n",
    "\n",
    "# Print out the tables, we should see the same weights as above\n",
    "print(\"Embedding Collection Table: \", embedding_collection.weight)\n",
    "print(\"Embedding Bag Collection Table: \", embedding_bag_collection.weight)\n",
    "\n",
    "# Lookup rows (ids for embedding ids) from the embedding tables\n",
    "# 2D tensor with shape (batch_size, ids for each batch)\n",
    "ids = torch.tensor([[1, 3]])\n",
    "print(\"Input row IDS: \", ids)\n",
    "\n",
    "embeddings = embedding_collection(ids)\n",
    "\n",
    "# Print out the embedding lookups\n",
    "# You should see the specific embeddings be the same as the rows (ids) of the embedding tables above\n",
    "print(\"Embedding Collection Results: \")\n",
    "print(embeddings)\n",
    "print(\"Shape: \", embeddings.shape)\n",
    "\n",
    "# ``nn.EmbeddingBag`` default pooling is mean, so should be mean of batch dimension of values above\n",
    "pooled_embeddings = embedding_bag_collection(ids)\n",
    "\n",
    "print(\"Embedding Bag Collection Results: \")\n",
    "print(pooled_embeddings)\n",
    "print(\"Shape: \", pooled_embeddings.shape)\n",
    "\n",
    "# ``nn.EmbeddingBag`` is the same as ``nn.Embedding`` but just with pooling (mean, sum, and so on)\n",
    "# We can see that the mean of the embeddings of embedding_collection is the same as the output of the embedding_bag_collection\n",
    "print(\"Mean: \", torch.mean(embedding_collection(ids), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchrec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchrec\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchrec'"
     ]
    }
   ],
   "source": [
    "import torchrec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isl-news-rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
